seed: 42
models:
  separate: false
  policy:
    class: GaussianMixin
    clip_actions: false
    clip_log_std: true
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.0
    network:
    - name: net
      input: STATES
      layers:
      - 512
      - 256
      - 128
      activations: elu
    output: ACTIONS
  value:
    class: DeterministicMixin
    clip_actions: false
    network:
    - name: net
      input: STATES
      layers:
      - 512
      - 256
      - 128
      activations: elu
    output: ONE
memory:
  class: RandomMemory
  memory_size: -1
agent:
  class: PPO
  rollouts: 24
  ppo_rollouts: 24
  ppo_learning_epochs: 5
  ppo_mini_batches: 4
  ppo_learning_rate: 0.001
  ppo_ratio_clip: 0.2
  ppo_value_clip: 0.2
  ppo_entropy_coef: 0.01
  ppo_value_coef: 1.0
  ppo_grad_norm_clip: 1.0
  a2c_rollouts: 24
  a2c_learning_rate: 0.001
  a2c_entropy_coef: 0.01
  a2c_value_coef: 1.0
  a2c_grad_norm_clip: 1.0
  discount_factor: 0.995
  lambda_gae: 0.95
  training_mode: adaptive
  ppo_weight: 0.7
  a2c_weight: 0.3
  adaptive_weighting: true
  performance_window: 10
  update_frequency:
    ppo: 1
    a2c: 1
  early_stopping_patience: 100
  early_stopping_threshold: 1e-2
  gradient_clip_norm: 1.0
  learning_rate_scheduler: null
  state_preprocessor: null
  state_preprocessor_kwargs: null
  value_preprocessor: null
  value_preprocessor_kwargs: null
  random_timesteps: 0
  learning_starts: 0
  rewards_shaper_scale: 1.0
  time_limit_bootstrap: false
  experiment:
    directory: /home/khan/isaaclab/logs/skrl/h1_rough_genuine_hybrid_ppo_a2c
    experiment_name: 2025-09-10_02-02-20_genuine_hybrid_ppo_a2c_torch
    write_interval: auto
    checkpoint_interval: 120
trainer:
  class: SequentialTrainer
  timesteps: 28800
  environment_info: log
  close_environment_at_exit: false
